{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d90cb548-c229-4b16-b1c9-17dd72f03829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "Model: unsloth/Qwen2.5-1.5B\n",
      "Max sequence length: 2048\n",
      "LoRA rank: 8\n",
      "Evaluation samples: 512\n",
      "SFT training samples: 1024\n",
      "RL training samples: 256\n",
      "Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "MODEL = \"unsloth/Qwen2.5-1.5B\"\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "LORA_RANK = 8\n",
    "EVAL_N = 512  # Number of examples to evaluate on\n",
    "SFT_MODEL_TRAIN_SAMPLES = 1024  # Full GSM8K train split\n",
    "RL_MODEL_TRAIN_SAMPLES = 256  # Subset for RL training\n",
    "BATCH_SIZE = 32  # Batch size for evaluation\n",
    "\n",
    "# Reasoning and solution tokens\n",
    "reasoning_start = \"<start_working_out>\"\n",
    "reasoning_end = \"<end_working_out>\"\n",
    "solution_start = \"<SOLUTION>\"\n",
    "solution_end = \"</SOLUTION>\"\n",
    "\n",
    "system_prompt = f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your working out.\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"Model: {MODEL}\")\n",
    "print(f\"Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"LoRA rank: {LORA_RANK}\")\n",
    "print(f\"Evaluation samples: {EVAL_N}\")\n",
    "print(f\"SFT training samples: {SFT_MODEL_TRAIN_SAMPLES}\")\n",
    "print(f\"RL training samples: {RL_MODEL_TRAIN_SAMPLES}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7934076-49bf-4f4e-8a22-de527f5c54df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 08-19 21:16:10 [__init__.py:235] Automatically detected platform cuda.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from transformers import TextStreamer\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from trl import SFTTrainer, SFTConfig, GRPOConfig, GRPOTrainer\n",
    "from vllm import SamplingParams\n",
    "import gc\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec3a6489-921c-424c-b79b-fbc002847d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, max_seq_length, lora_rank, load_in_4bit=False):\n",
    "    \"\"\"Load and prepare model for training/inference\"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    \n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_name,\n",
    "        max_seq_length = max_seq_length,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        fast_inference = True,\n",
    "        max_lora_rank = lora_rank,\n",
    "        gpu_memory_utilization = 0.7,\n",
    "    )\n",
    "\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r = lora_rank,\n",
    "        target_modules = [\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "        lora_alpha = lora_rank*2,\n",
    "        use_gradient_checkpointing = \"unsloth\",\n",
    "        random_state = 3407,\n",
    "    )\n",
    "    \n",
    "    # Set up chat template\n",
    "    chat_template = \\\n",
    "        \"{% if messages[0]['role'] == 'system' %}\"\\\n",
    "            \"{{ messages[0]['content'] + eos_token }}\"\\\n",
    "            \"{% set loop_messages = messages[1:] %}\"\\\n",
    "        \"{% else %}\"\\\n",
    "            \"{{ '{system_prompt}' + eos_token }}\"\\\n",
    "            \"{% set loop_messages = messages %}\"\\\n",
    "        \"{% endif %}\"\\\n",
    "        \"{% for message in loop_messages %}\"\\\n",
    "            \"{% if message['role'] == 'user' %}\"\\\n",
    "                \"{{ message['content'] }}\"\\\n",
    "            \"{% elif message['role'] == 'assistant' %}\"\\\n",
    "                \"{{ message['content'] + eos_token }}\"\\\n",
    "            \"{% endif %}\"\\\n",
    "        \"{% endfor %}\"\\\n",
    "        \"{% if add_generation_prompt %}{{ '{reasoning_start}' }}\"\\\n",
    "        \"{% endif %}\"\n",
    "\n",
    "    chat_template = chat_template.replace(\"'{system_prompt}'\", f\"'{system_prompt}'\").replace(\"'{reasoning_start}'\", f\"'{reasoning_start}'\")\n",
    "    tokenizer.chat_template = chat_template\n",
    "    \n",
    "    print(\"Model loaded successfully!\")\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88fbf91c-567c-48c8-a285-03f1d6a1decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hash_answer(text):\n",
    "    \"\"\"Extract answer from #### format\"\"\"\n",
    "    if \"####\" not in text: \n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "def extract_thinking(text):\n",
    "    \"\"\"Extract thinking between reasoning tokens\"\"\"\n",
    "    pattern = rf\"{re.escape(reasoning_start)}(.*?){re.escape(reasoning_end)}\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "def extract_solution(text):\n",
    "    \"\"\"Extract solution between solution tokens\"\"\"\n",
    "    pattern = rf\"{re.escape(solution_start)}(.*?){re.escape(solution_end)}\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10dd6622-fb42-4242-bf16-9dcafff7b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, dataset, eval_n, output_file, model_path=None):\n",
    "    \"\"\"Evaluate model on GSM8K test set using fast_generate with batching\"\"\"\n",
    "    print(f\"Evaluating model on {eval_n} examples with batch size {BATCH_SIZE}...\")\n",
    "    \n",
    "    # Load LoRA adapter if model_path is provided\n",
    "    lora_adapter = None\n",
    "    if model_path:\n",
    "        print(f\"Loading LoRA adapter from {model_path}\")\n",
    "        lora_adapter = model.load_lora(model_path)\n",
    "    \n",
    "    # Take subset for evaluation\n",
    "    eval_dataset = dataset.select(range(min(eval_n, len(dataset))))\n",
    "    \n",
    "    results = []\n",
    "    correct_count = 0\n",
    "    thinking_count = 0\n",
    "    extracted_answer_count = 0\n",
    "    \n",
    "    # Process in batches\n",
    "    for batch_start in range(0, len(eval_dataset), BATCH_SIZE):\n",
    "        batch_end = min(batch_start + BATCH_SIZE, len(eval_dataset))\n",
    "        batch = eval_dataset.select(range(batch_start, batch_end))\n",
    "        \n",
    "        print(f\"Processing batch {batch_start//BATCH_SIZE + 1}/{(len(eval_dataset)-1)//BATCH_SIZE + 1} (examples {batch_start}-{batch_end-1})\")\n",
    "        \n",
    "        # Prepare batch prompts\n",
    "        batch_prompts = []\n",
    "        batch_questions = []\n",
    "        batch_labeled_cots = []\n",
    "        batch_labeled_answers = []\n",
    "        \n",
    "        for example in batch:\n",
    "            question = example[\"question\"]\n",
    "            labeled_cot = example[\"answer\"]\n",
    "            labeled_answer = extract_hash_answer(labeled_cot)\n",
    "            \n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ]\n",
    "            \n",
    "            text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            batch_prompts.append(text)\n",
    "            batch_questions.append(question)\n",
    "            batch_labeled_cots.append(labeled_cot)\n",
    "            batch_labeled_answers.append(labeled_answer)\n",
    "        \n",
    "        # Generate responses using fast_generate\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            max_tokens=512,\n",
    "            stop=[tokenizer.eos_token],\n",
    "        )\n",
    "        \n",
    "        outputs = model.fast_generate(\n",
    "            batch_prompts,\n",
    "            sampling_params=sampling_params,\n",
    "            lora_request=lora_adapter,  # Use the LoRA adapter\n",
    "        )\n",
    "        \n",
    "        # Process batch results\n",
    "        for i, output in enumerate(outputs):\n",
    "            generated_text = output.outputs[0].text\n",
    "            \n",
    "            question = batch_questions[i]\n",
    "            labeled_cot = batch_labeled_cots[i]\n",
    "            labeled_answer = batch_labeled_answers[i]\n",
    "            \n",
    "            # Extract components\n",
    "            extracted_thinking = extract_thinking(reasoning_start + generated_text)\n",
    "            extracted_answer = extract_solution(generated_text)\n",
    "            \n",
    "            # Check correctness\n",
    "            correct = False\n",
    "            if extracted_answer and labeled_answer:\n",
    "                try:\n",
    "                    # Try numerical comparison\n",
    "                    extracted_num = float(extracted_answer.replace(\",\", \"\").strip())\n",
    "                    labeled_num = float(labeled_answer.replace(\",\", \"\").strip())\n",
    "                    correct = abs(extracted_num - labeled_num) < 1e-6\n",
    "                except:\n",
    "                    # Fall back to string comparison\n",
    "                    correct = extracted_answer.strip() == labeled_answer.strip()\n",
    "            \n",
    "            # Update counts\n",
    "            if correct:\n",
    "                correct_count += 1\n",
    "            if extracted_thinking:\n",
    "                thinking_count += 1\n",
    "            if extracted_answer:\n",
    "                extracted_answer_count += 1\n",
    "                \n",
    "            results.append({\n",
    "                \"question\": question,\n",
    "                \"labeled_cot\": labeled_cot,\n",
    "                \"labeled_answer\": labeled_answer,\n",
    "                \"generated_text\": generated_text,\n",
    "                \"extracted_thinking\": extracted_thinking,\n",
    "                \"extracted_answer\": extracted_answer,\n",
    "                \"correct\": correct\n",
    "            })\n",
    "    \n",
    "    # Save results\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print metrics\n",
    "    total = len(results)\n",
    "    accuracy = correct_count / total\n",
    "    thinking_prop = thinking_count / total\n",
    "    answer_prop = extracted_answer_count / total\n",
    "    \n",
    "    print(f\"\\nResults saved to {output_file}\")\n",
    "    print(f\"Accuracy: {accuracy:.3f} ({correct_count}/{total})\")\n",
    "    print(f\"Proportion with thinking: {thinking_prop:.3f} ({thinking_count}/{total})\")\n",
    "    print(f\"Proportion with extracted answer: {answer_prop:.3f} ({extracted_answer_count}/{total})\")\n",
    "    \n",
    "    return accuracy, thinking_prop, answer_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fffa51f-da23-4f03-96be-10137de185a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sft_dataset(dataset, n_samples, tokenizer):\n",
    "    \"\"\"Prepare dataset for SFT training\"\"\"\n",
    "    print(f\"Preparing SFT dataset with {n_samples} samples...\")\n",
    "    \n",
    "    # Take subset\n",
    "    train_data = dataset.select(range(min(n_samples, len(dataset))))\n",
    "    \n",
    "    def format_example(example):\n",
    "        question = example[\"question\"]\n",
    "        answer_text = example[\"answer\"]\n",
    "        \n",
    "        # Extract the numerical answer\n",
    "        numerical_answer = extract_hash_answer(answer_text)\n",
    "        if not numerical_answer:\n",
    "            return None\n",
    "            \n",
    "        # Create formatted response with thinking and solution\n",
    "        # Use the step-by-step solution as thinking\n",
    "        thinking = answer_text.split(\"####\")[0].strip()\n",
    "        \n",
    "        response = f\"{reasoning_start}well...{thinking}{reasoning_end}{solution_start}{numerical_answer}{solution_end}\"\n",
    "        \n",
    "        return {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "                {\"role\": \"assistant\", \"content\": response}\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    # Format examples\n",
    "    formatted_data = []\n",
    "    for example in train_data:\n",
    "        formatted = format_example(example)\n",
    "        if formatted:\n",
    "            formatted_data.append(formatted)\n",
    "    \n",
    "    # Convert to text format\n",
    "    for item in formatted_data:\n",
    "        item[\"text\"] = tokenizer.apply_chat_template(item[\"messages\"], tokenize=False)\n",
    "    \n",
    "    dataset_dict = {\n",
    "        \"messages\": [item[\"messages\"] for item in formatted_data],\n",
    "        \"text\": [item[\"text\"] for item in formatted_data]\n",
    "    }\n",
    "    \n",
    "    return Dataset.from_dict(dataset_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d0a492a-6a4e-4977-b149-cb4e99e2b6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sft_model(model, tokenizer, train_dataset, output_dir=\"sft_model\", learning_rate=2e-4):\n",
    "    \"\"\"Train model using SFT\"\"\"\n",
    "    print(\"Starting SFT training...\")\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        args=SFTConfig(\n",
    "            dataset_text_field=\"text\",\n",
    "            per_device_train_batch_size=4,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=10,\n",
    "            num_train_epochs=1,\n",
    "            learning_rate=2e-4,\n",
    "            logging_steps=10,\n",
    "            optim=\"adamw_8bit\",\n",
    "            weight_decay=0.01,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            seed=3407,\n",
    "            report_to=\"none\",\n",
    "            output_dir=output_dir,\n",
    "            save_steps=500,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    print(f\"SFT model saved to {output_dir}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc63c111-db43-4653-849f-8ad899b6e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_grpo_dataset(dataset, n_samples):\n",
    "    \"\"\"Prepare dataset for GRPO training\"\"\"\n",
    "    print(f\"Preparing GRPO dataset with {n_samples} samples...\")\n",
    "    \n",
    "    train_data = dataset.select(range(min(n_samples, len(dataset))))\n",
    "    \n",
    "    def format_for_grpo(example):\n",
    "        question = example[\"question\"]\n",
    "        answer = extract_hash_answer(example[\"answer\"])\n",
    "        \n",
    "        return {\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ],\n",
    "            \"answer\": answer\n",
    "        }\n",
    "    \n",
    "    formatted_data = []\n",
    "    for example in train_data:\n",
    "        formatted = format_for_grpo(example)\n",
    "        if formatted[\"answer\"]:\n",
    "            formatted_data.append(formatted)\n",
    "    \n",
    "    return Dataset.from_list(formatted_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "514bb19b-ae92-44e9-a215-7cc870a80b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward functions for GRPO\n",
    "def match_format_exactly(completions, **kwargs):\n",
    "    solution_end_regex = r\"</SOLUTION>[\\s]{0,}\"\n",
    "    match_format = re.compile(\n",
    "        rf\"{re.escape(reasoning_end)}.*?{re.escape(solution_start)}(.+?){solution_end_regex}[\\s]{{0,}}$\",\n",
    "        flags=re.MULTILINE | re.DOTALL\n",
    "    )\n",
    "    \n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        response = completion[0][\"content\"]\n",
    "        score = 3.0 if match_format.search(response) else 0.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def match_format_approximately(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        response = completion[0][\"content\"]\n",
    "        score = 0\n",
    "        score += 0.5 if response.count(reasoning_end) == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_start) == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_end) == 1 else -1.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    solution_end_regex = r\"</SOLUTION>[\\s]{0,}\"\n",
    "    match_format = re.compile(\n",
    "        rf\"{re.escape(reasoning_end)}.*?{re.escape(solution_start)}(.+?){solution_end_regex}\",\n",
    "        flags=re.MULTILINE | re.DOTALL\n",
    "    )\n",
    "    \n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    extracted_responses = [\n",
    "        match.group(1).strip() if (match := match_format.search(r)) else None\n",
    "        for r in responses\n",
    "    ]\n",
    "    \n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        if guess is None:\n",
    "            scores.append(-2.0)\n",
    "            continue\n",
    "            \n",
    "        if guess == true_answer:\n",
    "            score = 5.0\n",
    "        elif guess.strip() == true_answer.strip():\n",
    "            score = 3.5\n",
    "        else:\n",
    "            try:\n",
    "                ratio = float(guess) / float(true_answer)\n",
    "                if 0.9 <= ratio <= 1.1:\n",
    "                    score = 2.0\n",
    "                elif 0.8 <= ratio <= 1.2:\n",
    "                    score = 1.5\n",
    "                else:\n",
    "                    score = -2.5\n",
    "            except:\n",
    "                score = -4.5\n",
    "        scores.append(score)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdf29b5c-57cd-4858-8892-3bd8f9558aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_grpo_model(model, tokenizer, train_dataset, output_dir=\"grpo_model\"):\n",
    "    \"\"\"Train model using GRPO\"\"\"\n",
    "    print(\"Starting GRPO training...\")\n",
    "    \n",
    "    # Calculate max prompt length\n",
    "    tokenized = train_dataset.map(\n",
    "        lambda x: {\"tokens\": tokenizer.apply_chat_template(x[\"prompt\"], add_generation_prompt=True, tokenize=True)},\n",
    "        batched=True,\n",
    "    )\n",
    "    max_prompt_length = int(np.quantile([len(tokens) for tokens in tokenized[\"tokens\"]], 0.9)) + 1\n",
    "    max_completion_length = MAX_SEQ_LENGTH - max_prompt_length\n",
    "    \n",
    "    print(f\"Max prompt length: {max_prompt_length}\")\n",
    "    print(f\"Max completion length: {max_completion_length}\")\n",
    "    \n",
    "    vllm_sampling_params = SamplingParams(\n",
    "        min_p=0.1,\n",
    "        top_p=1.0,\n",
    "        top_k=-1,\n",
    "        seed=3407,\n",
    "        stop=[tokenizer.eos_token],\n",
    "        include_stop_str_in_output=True,\n",
    "    )\n",
    "    \n",
    "    training_args = GRPOConfig(\n",
    "        vllm_sampling_params=vllm_sampling_params,\n",
    "        temperature=1.0,\n",
    "        learning_rate=5e-6,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        optim=\"adamw_8bit\",\n",
    "        logging_steps=5,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        num_generations=2,\n",
    "        max_prompt_length=max_prompt_length,\n",
    "        max_completion_length=max_completion_length,\n",
    "        max_steps=200,\n",
    "        save_steps=100,\n",
    "        report_to=\"none\",\n",
    "        output_dir=output_dir,\n",
    "    )\n",
    "    \n",
    "    trainer = GRPOTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        reward_funcs=[\n",
    "            match_format_exactly,\n",
    "            match_format_approximately,\n",
    "            check_answer,\n",
    "        ],\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    print(f\"GRPO model saved to {output_dir}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cd97c14-2da4-4a86-a5c4-000fa28b3f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GSM8K dataset...\n",
      "GSM8K train size: 7473\n",
      "GSM8K test size: 1319\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "print(\"Loading GSM8K dataset...\")\n",
    "gsm8k_train = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n",
    "gsm8k_test = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
    "\n",
    "print(f\"GSM8K train size: {len(gsm8k_train)}\")\n",
    "print(f\"GSM8K test size: {len(gsm8k_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fd662f6-8987-4499-ab3a-189f46e172b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "STEP 1: EVALUATING BASE MODEL\n",
      "==================================================\n",
      "Loading model: unsloth/Qwen2.5-1.5B\n",
      "Unsloth: Patching vLLM v1 graph capture\n",
      "Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2025.8.6: Fast Qwen2 patching. Transformers: 4.55.2. vLLM: 0.10.0.\n",
      "   \\\\   /|    NVIDIA RTX 4000 Ada Generation. Num GPUs = 1. Max memory: 19.674 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/Qwen2.5-1.5B with actual GPU utilization = 69.29%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 19.67 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 224.\n",
      "Unsloth: vLLM's KV Cache can use up to 10.74 GB. Also swap space = 6 GB.\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 08-19 16:33:41 [config.py:1604] Using max model len 2048\n",
      "INFO 08-19 16:33:44 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 08-19 16:33:46 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-1.5B', speculative_config=None, tokenizer='unsloth/Qwen2.5-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":true,\"trace.graph_diagram\":false,\"compile_threads\":32,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":448,\"local_cache_dir\":null}\n",
      "INFO 08-19 16:33:46 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 08-19 16:33:46 [topk_topp_sampler.py:36] FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.\n",
      "INFO 08-19 16:33:46 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-1.5B...\n",
      "INFO 08-19 16:33:47 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 08-19 16:33:47 [cuda.py:246] Using FlashInfer backend on V1 engine.\n",
      "INFO 08-19 16:33:47 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "INFO 08-19 16:33:48 [weight_utils.py:349] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2c410b4d5f4c2ab06d5b5fdf8fe97a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-19 16:33:48 [default_loader.py:262] Loading weights took 0.56 seconds\n",
      "INFO 08-19 16:33:48 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 08-19 16:33:49 [gpu_model_runner.py:1892] Model loading took 2.9034 GiB and 1.887167 seconds\n",
      "INFO 08-19 16:34:06 [backends.py:530] Using cache directory: /root/.cache/vllm/torch_compile_cache/7d34cb9709/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 08-19 16:34:06 [backends.py:541] Dynamo bytecode transform time: 16.37 s\n",
      "INFO 08-19 16:34:14 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.260 s\n",
      "INFO 08-19 16:34:18 [monitor.py:34] torch.compile takes 16.37 s in total\n",
      "INFO 08-19 16:34:19 [gpu_worker.py:255] Available KV cache memory: 9.48 GiB\n",
      "INFO 08-19 16:34:20 [kv_cache_utils.py:833] GPU KV cache size: 355,040 tokens\n",
      "INFO 08-19 16:34:20 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 173.36x\n",
      "INFO 08-19 16:34:20 [vllm_utils.py:643] Unsloth: Running patched vLLM v1 `capture_model`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 59/59 [00:20<00:00,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-19 16:34:40 [gpu_model_runner.py:2485] Graph capturing finished in 21 secs, took 0.55 GiB\n",
      "INFO 08-19 16:34:40 [vllm_utils.py:650] Unsloth: Patched vLLM v1 graph capture finished in 21 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-19 16:34:41 [core.py:193] init engine (profile, create kv cache, warmup model) took 52.28 seconds\n",
      "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'k_norm', 'pre_feedforward_layernorm', 'q_norm']\n",
      "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'k_norm', 'pre_feedforward_layernorm', 'q_norm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.8.6 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"STEP 1: EVALUATING BASE MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "base_model, tokenizer = load_model(MODEL, MAX_SEQ_LENGTH, LORA_RANK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53758e69-2c2a-4842-84f2-5fdd95aae3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on 50 examples with batch size 32...\n",
      "Processing batch 1/2 (examples 0-31)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2bd51d7d9964500b8cbe0afaaca0f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35bd46c3c94c467ba55048357a9ab675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 2/2 (examples 32-49)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07e28af8c6a4ceda902e894230e0c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ecd638e2a1e4c6d8a6b4ce8d3f227d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/18 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to base_model_eval.csv\n",
      "Accuracy: 0.020 (1/50)\n",
      "Proportion with thinking: 0.500 (25/50)\n",
      "Proportion with extracted answer: 0.180 (9/50)\n",
      "\n",
      "Base model results:\n",
      "Accuracy: 0.020\n",
      "Thinking proportion: 0.500\n",
      "Answer proportion: 0.180\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate base model\n",
    "base_accuracy, base_thinking_prop, base_answer_prop = evaluate_model(\n",
    "    base_model, tokenizer, gsm8k_test, EVAL_N, \"base_model_eval.csv\"\n",
    ")\n",
    "\n",
    "print(f\"\\nBase model results:\")\n",
    "print(f\"Accuracy: {base_accuracy:.3f}\")\n",
    "print(f\"Thinking proportion: {base_thinking_prop:.3f}\")\n",
    "print(f\"Answer proportion: {base_answer_prop:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "201ce11e-9ca5-4af3-b258-74e15d4d3262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 2: SFT TRAINING\n",
      "==================================================\n",
      "Preparing SFT dataset with 1024 samples...\n",
      "SFT dataset size: 1024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<start_working_out>well...Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.<end_working_out><SOLUTION>72</SOLUTION>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: SFT training\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 2: SFT TRAINING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Prepare SFT dataset\n",
    "sft_dataset = prepare_sft_dataset(gsm8k_train, SFT_MODEL_TRAIN_SAMPLES, tokenizer)\n",
    "print(f\"SFT dataset size: {len(sft_dataset)}\")\n",
    "sft_dataset[0]['messages'][-1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bd51025-00d0-4bdf-a66c-038ec3d4d92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SFT training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e1f75dabbe4f7784c94ceb6c8e2c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/1024 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,024 | Num Epochs = 1 | Total steps = 64\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 9,232,384 of 1,552,946,688 (0.59% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 01:21, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.059800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.421800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.255900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.237500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.253300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.227000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT model saved to sft_model\n"
     ]
    }
   ],
   "source": [
    "# Train SFT model\n",
    "sft_model = train_sft_model(base_model, tokenizer, sft_dataset, \"sft_model\", learning_rate=2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "398a0dbb-5d6e-4b8a-98fc-1e532ca042e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98051eb1a934ea39326fefad471bbe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76a7e618f414111b8f8f526bb5f0ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                         | 0/1 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "well...The equation (x + 2)^2 = 0 can be solved by taking the square root of both sides, giving x + 2 = 0.\n",
      "Then, solving for x, we have x = -2.<end_working_out><SOLUTION>-2</SOLUTION>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Solve (x + 2)^2 = 0.\"}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    top_k=20,\n",
    "    max_tokens=256,\n",
    "    stop=[tokenizer.eos_token],\n",
    ")\n",
    "\n",
    "lora_adapter = sft_model.load_lora(\"sft_model\")\n",
    "\n",
    "outputs = sft_model.fast_generate(\n",
    "    [text],  # Note: needs to be a list even for single input\n",
    "    sampling_params=sampling_params,\n",
    "    lora_request=lora_adapter,\n",
    ")\n",
    "\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dde5adcf-544d-45e9-bc34-91d57c15690b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating SFT model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sft_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Evaluate SFT model\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvaluating SFT model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m sft_accuracy, sft_thinking_prop, sft_answer_prop = evaluate_model(\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43msft_model\u001b[49m, tokenizer, gsm8k_test, EVAL_N, \u001b[33m\"\u001b[39m\u001b[33msft_model_eval.csv\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msft_model\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSFT model results:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msft_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'sft_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate SFT model\n",
    "print(\"\\nEvaluating SFT model...\")\n",
    "sft_accuracy, sft_thinking_prop, sft_answer_prop = evaluate_model(\n",
    "    sft_model, tokenizer, gsm8k_test, EVAL_N, \"sft_model_eval.csv\", \"sft_model\"\n",
    ")\n",
    "\n",
    "print(f\"\\nSFT model results:\")\n",
    "print(f\"Accuracy: {sft_accuracy:.3f}\")\n",
    "print(f\"Thinking proportion: {sft_thinking_prop:.3f}\")\n",
    "print(f\"Answer proportion: {sft_answer_prop:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "095fc7a6-8f06-4da5-a793-f8937e8f7a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.00 GB\n",
      "Cached: 0.00 GB\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "Max allocated: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Clean up memory\n",
    "del sft_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# Reset all memory stats\n",
    "torch.cuda.reset_accumulated_memory_stats()\n",
    "\n",
    "\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"Cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "# More detailed info\n",
    "print(torch.cuda.memory_summary())\n",
    "\n",
    "# Get max memory used\n",
    "print(f\"Max allocated: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc5d8933-8f0b-4d4b-baea-2910069c636f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 3: RL TRAINING (SFT + GRPO)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 3: RL training (SFT + GRPO)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 3: RL TRAINING (SFT + GRPO)\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cd890dc-5dd7-46cc-ac4b-1d9c9313dddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fresh base model for RL training...\n",
      "Loading model: unsloth/Qwen2.5-1.5B\n",
      "Unsloth: Patching vLLM v1 graph capture\n",
      "Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2025.8.6: Fast Qwen2 patching. Transformers: 4.55.2. vLLM: 0.10.0.\n",
      "   \\\\   /|    NVIDIA RTX 4000 Ada Generation. Num GPUs = 1. Max memory: 19.674 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/Qwen2.5-1.5B with actual GPU utilization = 69.29%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 19.67 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 224.\n",
      "Unsloth: vLLM's KV Cache can use up to 10.74 GB. Also swap space = 6 GB.\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 08-19 16:39:29 [config.py:1604] Using max model len 2048\n",
      "INFO 08-19 16:39:31 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 08-19 16:39:32 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-1.5B', speculative_config=None, tokenizer='unsloth/Qwen2.5-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":true,\"trace.graph_diagram\":false,\"compile_threads\":32,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":448,\"local_cache_dir\":null}\n",
      "INFO 08-19 16:39:33 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 08-19 16:39:33 [topk_topp_sampler.py:36] FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.\n",
      "INFO 08-19 16:39:33 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-1.5B...\n",
      "INFO 08-19 16:39:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 08-19 16:39:33 [cuda.py:246] Using FlashInfer backend on V1 engine.\n",
      "INFO 08-19 16:39:34 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "INFO 08-19 16:39:34 [weight_utils.py:349] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e849560a112449a19d4957cfcdcd7a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-19 16:39:35 [default_loader.py:262] Loading weights took 0.55 seconds\n",
      "INFO 08-19 16:39:35 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 08-19 16:39:36 [gpu_model_runner.py:1892] Model loading took 2.9034 GiB and 1.605695 seconds\n",
      "INFO 08-19 16:39:50 [backends.py:530] Using cache directory: /root/.cache/vllm/torch_compile_cache/7d34cb9709/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 08-19 16:39:50 [backends.py:541] Dynamo bytecode transform time: 13.80 s\n",
      "INFO 08-19 16:39:58 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.297 s\n",
      "INFO 08-19 16:40:13 [monitor.py:34] torch.compile takes 13.80 s in total\n",
      "INFO 08-19 16:40:14 [gpu_worker.py:255] Available KV cache memory: 9.48 GiB\n",
      "INFO 08-19 16:40:15 [kv_cache_utils.py:833] GPU KV cache size: 354,896 tokens\n",
      "INFO 08-19 16:40:15 [kv_cache_utils.py:837] Maximum concurrency for 2,048 tokens per request: 173.29x\n",
      "INFO 08-19 16:40:15 [vllm_utils.py:643] Unsloth: Running patched vLLM v1 `capture_model`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 59/59 [00:22<00:00,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-19 16:40:37 [gpu_model_runner.py:2485] Graph capturing finished in 22 secs, took 0.55 GiB\n",
      "INFO 08-19 16:40:37 [vllm_utils.py:650] Unsloth: Patched vLLM v1 graph capture finished in 22 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-19 16:40:38 [core.py:193] init engine (profile, create kv cache, warmup model) took 62.15 seconds\n",
      "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'q_norm', 'pre_feedforward_layernorm', 'k_norm']\n",
      "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'q_norm', 'pre_feedforward_layernorm', 'k_norm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.8.6 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading fresh base model for RL training...\")\n",
    "rl_base_model, rl_tokenizer = load_model(MODEL, MAX_SEQ_LENGTH, LORA_RANK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08942513-6441-48b3-a4e5-f51880ac6917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doing SFT on 256 samples...\n",
      "Preparing SFT dataset with 256 samples...\n",
      "Starting SFT training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add6f78b8cfb4b12bb1ed42f532507e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 256 | Num Epochs = 1 | Total steps = 16\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 9,232,384 of 1,552,946,688 (0.59% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.003200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT model saved to rl_sft_model\n"
     ]
    }
   ],
   "source": [
    "# First do SFT on subset\n",
    "print(f\"\\nDoing SFT on {RL_MODEL_TRAIN_SAMPLES} samples...\")\n",
    "rl_sft_dataset = prepare_sft_dataset(gsm8k_train, RL_MODEL_TRAIN_SAMPLES, rl_tokenizer)\n",
    "rl_sft_model = train_sft_model(rl_base_model, rl_tokenizer, rl_sft_dataset, \"rl_sft_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e6ff6cb-f6d8-412d-aeef-ab6f7a004778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating RL-SFT model...\n",
      "Evaluating model on 512 examples with batch size 32...\n",
      "Loading LoRA adapter from rl_sft_model\n",
      "Processing batch 1/16 (examples 0-31)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d735615087854989b481f8e144018914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f200ccff4ad947f29ceb2722838e6662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 2/16 (examples 32-63)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e925a6fa7c54320aa4f644f118aaee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfff1ba936f0432b840fe6edd9dfad8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 3/16 (examples 64-95)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc7636c6538475c897382efca3f81a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554c6b1022ed42c8b1f14b456d918cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 4/16 (examples 96-127)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ff59490427463fa7a7a38befa790dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844bc8e33f1d47a1adf7d39a4a84eddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 5/16 (examples 128-159)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542a5930e6234d2aa89ea407feb4954f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4665d0e6fde4c1db0670bcee7c8fe9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 6/16 (examples 160-191)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d89ee09a2dd4a31a08730498bc80814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e996f1045b4f91a0a49fa1c02bfdee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 7/16 (examples 192-223)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a7ce65828d45e0b80a3bec417b584d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89fba0aede84f4ebb329bff6b44e486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 8/16 (examples 224-255)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee353871c9494778a22c1f984de6529b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02278bd4de64466a9a68333478041d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 9/16 (examples 256-287)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d594f8cd1c945b68ed553b468ddc35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd177a3c4ab44099c1db938a68b8f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 10/16 (examples 288-319)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e99e66e9094467bbfb6a27c36eddf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4923600bef084ff88139081f997adff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 11/16 (examples 320-351)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d0981df78f4efebe56b847bc15d14a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc60497cbf948b38ee0bf82ef43f091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 12/16 (examples 352-383)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08a3191668a4605bfabcdf1287e165f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980766789794429085bb32410ed0a440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 13/16 (examples 384-415)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f586c863e7ef4f34891195419265564f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da358741543641e98e0ffe49101a319c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 14/16 (examples 416-447)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17d63165f5e40f283e35475f1d3cdec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43cfcc810e846d0a655a9961e2d852c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 15/16 (examples 448-479)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453c389cd8a848e888211b8066a91c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9895536f6771439fabbe69a76ac113a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 16/16 (examples 480-511)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c06447796640caa2caf2c0500d6cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcfb3d8d60614523b2c0338c90cd8b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to rl-sft_model_eval.csv\n",
      "Accuracy: 0.186 (95/512)\n",
      "Proportion with thinking: 0.770 (394/512)\n",
      "Proportion with extracted answer: 0.695 (356/512)\n",
      "\n",
      "RL-SFT model results:\n",
      "Accuracy: 0.186\n",
      "Thinking proportion: 0.770\n",
      "Answer proportion: 0.695\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluating RL-SFT model...\")\n",
    "rl_sft_accuracy, rl_srl_ft_thinking_prop, rl_sft_answer_prop = evaluate_model(\n",
    "    rl_sft_model, rl_tokenizer, gsm8k_test, EVAL_N, \"rl-sft_model_eval.csv\", \"rl_sft_model\"\n",
    ")\n",
    "\n",
    "print(f\"\\nRL-SFT model results:\")\n",
    "print(f\"Accuracy: {rl_sft_accuracy:.3f}\")\n",
    "print(f\"Thinking proportion: {rl_srl_ft_thinking_prop:.3f}\")\n",
    "print(f\"Answer proportion: {rl_sft_answer_prop:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "883fafb6-f2c2-4d2b-853e-5ab340029bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doing GRPO training...\n",
      "Preparing GRPO dataset with 256 samples...\n",
      "Starting GRPO training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60074619884d418eb7044f310243aa85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max prompt length: 138\n",
      "Max completion length: 1910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 256 | Num Epochs = 2 | Total steps = 200\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 2 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 9,232,384 of 1,552,946,688 (0.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 14:24, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completions / mean_length</th>\n",
       "      <th>completions / min_length</th>\n",
       "      <th>completions / max_length</th>\n",
       "      <th>completions / clipped_ratio</th>\n",
       "      <th>completions / mean_terminated_length</th>\n",
       "      <th>completions / min_terminated_length</th>\n",
       "      <th>completions / max_terminated_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>entropy</th>\n",
       "      <th>rewards / match_format_exactly / mean</th>\n",
       "      <th>rewards / match_format_exactly / std</th>\n",
       "      <th>rewards / match_format_approximately / mean</th>\n",
       "      <th>rewards / match_format_approximately / std</th>\n",
       "      <th>rewards / check_answer / mean</th>\n",
       "      <th>rewards / check_answer / std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>-1.650000</td>\n",
       "      <td>2.192031</td>\n",
       "      <td>164.600000</td>\n",
       "      <td>38.400000</td>\n",
       "      <td>398.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>164.600000</td>\n",
       "      <td>38.400000</td>\n",
       "      <td>398.800000</td>\n",
       "      <td>0.414518</td>\n",
       "      <td>0</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>1.339230</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>1.445201</td>\n",
       "      <td>-2.325000</td>\n",
       "      <td>1.547021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>3.464823</td>\n",
       "      <td>82.850000</td>\n",
       "      <td>34.800000</td>\n",
       "      <td>151.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>82.850000</td>\n",
       "      <td>34.800000</td>\n",
       "      <td>151.800000</td>\n",
       "      <td>0.352510</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>1.592820</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-1.275000</td>\n",
       "      <td>2.384665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>2.934493</td>\n",
       "      <td>115.650000</td>\n",
       "      <td>19.600000</td>\n",
       "      <td>233.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>115.650000</td>\n",
       "      <td>19.600000</td>\n",
       "      <td>233.600000</td>\n",
       "      <td>0.226395</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.292820</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.987298</td>\n",
       "      <td>-1.425000</td>\n",
       "      <td>1.561355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>79.550000</td>\n",
       "      <td>31.400000</td>\n",
       "      <td>156.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>79.550000</td>\n",
       "      <td>31.400000</td>\n",
       "      <td>156.400000</td>\n",
       "      <td>0.191723</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>1.592820</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.461492</td>\n",
       "      <td>-2.350000</td>\n",
       "      <td>1.732986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>-0.275000</td>\n",
       "      <td>2.863782</td>\n",
       "      <td>86.450000</td>\n",
       "      <td>23.800000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>86.450000</td>\n",
       "      <td>23.800000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>0.224326</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>1.339230</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>1.432177</td>\n",
       "      <td>-2.450000</td>\n",
       "      <td>1.588731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>84.200000</td>\n",
       "      <td>39.600000</td>\n",
       "      <td>154.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84.200000</td>\n",
       "      <td>39.600000</td>\n",
       "      <td>154.400000</td>\n",
       "      <td>0.442272</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.292820</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.039230</td>\n",
       "      <td>-1.700000</td>\n",
       "      <td>2.289900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>3.111270</td>\n",
       "      <td>98.550000</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>161.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98.550000</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>161.800000</td>\n",
       "      <td>0.243146</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.546410</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.974526</td>\n",
       "      <td>-1.975000</td>\n",
       "      <td>2.091099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>3.429468</td>\n",
       "      <td>95.350000</td>\n",
       "      <td>41.200000</td>\n",
       "      <td>197.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>95.350000</td>\n",
       "      <td>41.200000</td>\n",
       "      <td>197.400000</td>\n",
       "      <td>0.266440</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>1.592820</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>1.292610</td>\n",
       "      <td>-1.875000</td>\n",
       "      <td>2.567301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>2.687006</td>\n",
       "      <td>183.500000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>465.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>183.500000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>465.800000</td>\n",
       "      <td>0.145043</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.385641</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.233638</td>\n",
       "      <td>-1.975000</td>\n",
       "      <td>2.087912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>-1.400000</td>\n",
       "      <td>1.838478</td>\n",
       "      <td>63.600000</td>\n",
       "      <td>17.800000</td>\n",
       "      <td>122.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63.600000</td>\n",
       "      <td>17.800000</td>\n",
       "      <td>122.200000</td>\n",
       "      <td>0.235480</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.546410</td>\n",
       "      <td>-0.075000</td>\n",
       "      <td>1.838984</td>\n",
       "      <td>-2.225000</td>\n",
       "      <td>0.982004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>3.394113</td>\n",
       "      <td>116.750000</td>\n",
       "      <td>32.200000</td>\n",
       "      <td>214.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>116.750000</td>\n",
       "      <td>32.200000</td>\n",
       "      <td>214.200000</td>\n",
       "      <td>0.437898</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>1.592820</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>1.712189</td>\n",
       "      <td>-1.975000</td>\n",
       "      <td>2.202401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>-0.975000</td>\n",
       "      <td>2.722361</td>\n",
       "      <td>74.750000</td>\n",
       "      <td>35.400000</td>\n",
       "      <td>116.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>74.750000</td>\n",
       "      <td>35.400000</td>\n",
       "      <td>116.600000</td>\n",
       "      <td>0.360558</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.385641</td>\n",
       "      <td>-0.525000</td>\n",
       "      <td>1.948861</td>\n",
       "      <td>-1.650000</td>\n",
       "      <td>1.988089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>3.924443</td>\n",
       "      <td>163.150000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>389.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>163.150000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>389.400000</td>\n",
       "      <td>0.239136</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.474456</td>\n",
       "      <td>-0.725000</td>\n",
       "      <td>3.324452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>2.651650</td>\n",
       "      <td>80.200000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>80.200000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>0.187280</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>1.246410</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.987298</td>\n",
       "      <td>-1.125000</td>\n",
       "      <td>2.042602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>-0.450000</td>\n",
       "      <td>2.757716</td>\n",
       "      <td>155.250000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>453.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>155.250000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>453.400000</td>\n",
       "      <td>0.383218</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>1.339230</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>1.359669</td>\n",
       "      <td>-2.175000</td>\n",
       "      <td>1.403783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>3.853732</td>\n",
       "      <td>201.700000</td>\n",
       "      <td>48.400000</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>201.700000</td>\n",
       "      <td>48.400000</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>0.223245</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.639230</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.302851</td>\n",
       "      <td>-1.550000</td>\n",
       "      <td>2.730170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>4.101219</td>\n",
       "      <td>121.650000</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>253.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.650000</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>253.400000</td>\n",
       "      <td>0.236825</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>1.246410</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>1.419213</td>\n",
       "      <td>-1.025000</td>\n",
       "      <td>2.746869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-0.775000</td>\n",
       "      <td>2.863782</td>\n",
       "      <td>333.650000</td>\n",
       "      <td>48.400000</td>\n",
       "      <td>652.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>166.450000</td>\n",
       "      <td>48.400000</td>\n",
       "      <td>349.800000</td>\n",
       "      <td>0.145575</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.639230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.794938</td>\n",
       "      <td>-1.975000</td>\n",
       "      <td>1.695866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>2.262742</td>\n",
       "      <td>67.450000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>140.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>67.450000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>140.800000</td>\n",
       "      <td>0.633347</td>\n",
       "      <td>No Log</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>1.546410</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>1.223205</td>\n",
       "      <td>-2.125000</td>\n",
       "      <td>1.299046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>-0.650000</td>\n",
       "      <td>3.394112</td>\n",
       "      <td>127.650000</td>\n",
       "      <td>56.400000</td>\n",
       "      <td>231.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>127.650000</td>\n",
       "      <td>56.400000</td>\n",
       "      <td>231.600000</td>\n",
       "      <td>0.527369</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>-0.450000</td>\n",
       "      <td>2.016320</td>\n",
       "      <td>-1.100000</td>\n",
       "      <td>2.051632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>-0.550000</td>\n",
       "      <td>2.899138</td>\n",
       "      <td>89.750000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>89.750000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>0.315344</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>1.592820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.632166</td>\n",
       "      <td>-2.200000</td>\n",
       "      <td>2.067600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.325000</td>\n",
       "      <td>3.570889</td>\n",
       "      <td>92.600000</td>\n",
       "      <td>35.200000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>92.600000</td>\n",
       "      <td>35.200000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>0.392998</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>1.060433</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2.153917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>4.384062</td>\n",
       "      <td>137.600000</td>\n",
       "      <td>20.600000</td>\n",
       "      <td>298.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>137.600000</td>\n",
       "      <td>20.600000</td>\n",
       "      <td>298.000000</td>\n",
       "      <td>0.220080</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>1.339230</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>1.711562</td>\n",
       "      <td>-1.100000</td>\n",
       "      <td>2.929253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>2.425000</td>\n",
       "      <td>3.924443</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>60.600000</td>\n",
       "      <td>159.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>60.600000</td>\n",
       "      <td>159.800000</td>\n",
       "      <td>0.310580</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>1.246410</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.764564</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>3.226567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>2.085965</td>\n",
       "      <td>80.300000</td>\n",
       "      <td>26.800000</td>\n",
       "      <td>143.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>80.300000</td>\n",
       "      <td>26.800000</td>\n",
       "      <td>143.400000</td>\n",
       "      <td>0.253206</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.639230</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.510433</td>\n",
       "      <td>-0.525000</td>\n",
       "      <td>3.106147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>3.005204</td>\n",
       "      <td>126.700000</td>\n",
       "      <td>39.400000</td>\n",
       "      <td>220.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>126.700000</td>\n",
       "      <td>39.400000</td>\n",
       "      <td>220.200000</td>\n",
       "      <td>0.313683</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.292820</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.219475</td>\n",
       "      <td>-1.800000</td>\n",
       "      <td>2.064248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>3.394112</td>\n",
       "      <td>92.950000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>92.950000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>1.170815</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.292820</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.320937</td>\n",
       "      <td>-1.325000</td>\n",
       "      <td>1.996338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>3.924443</td>\n",
       "      <td>83.700000</td>\n",
       "      <td>27.400000</td>\n",
       "      <td>160.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>83.700000</td>\n",
       "      <td>27.400000</td>\n",
       "      <td>160.800000</td>\n",
       "      <td>0.421439</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.546410</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.569615</td>\n",
       "      <td>-1.400000</td>\n",
       "      <td>2.988789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>4.207285</td>\n",
       "      <td>91.500000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>149.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>91.500000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>149.200000</td>\n",
       "      <td>0.384254</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>1.592820</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.320937</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>2.715213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>3.288047</td>\n",
       "      <td>163.900000</td>\n",
       "      <td>51.400000</td>\n",
       "      <td>320.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>163.900000</td>\n",
       "      <td>51.400000</td>\n",
       "      <td>320.200000</td>\n",
       "      <td>0.686100</td>\n",
       "      <td>No Log</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>1.292820</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.264720</td>\n",
       "      <td>-1.725000</td>\n",
       "      <td>2.423284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>2.580940</td>\n",
       "      <td>92.900000</td>\n",
       "      <td>32.400000</td>\n",
       "      <td>163.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>92.900000</td>\n",
       "      <td>32.400000</td>\n",
       "      <td>163.800000</td>\n",
       "      <td>0.310198</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>1.592820</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>1.502018</td>\n",
       "      <td>-1.650000</td>\n",
       "      <td>1.857536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>-0.475000</td>\n",
       "      <td>2.793072</td>\n",
       "      <td>128.400000</td>\n",
       "      <td>24.800000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>128.400000</td>\n",
       "      <td>24.800000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>0.288706</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1.292820</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>1.968336</td>\n",
       "      <td>-2.350000</td>\n",
       "      <td>1.558124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>3.853732</td>\n",
       "      <td>102.950000</td>\n",
       "      <td>36.800000</td>\n",
       "      <td>208.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>102.950000</td>\n",
       "      <td>36.800000</td>\n",
       "      <td>208.400000</td>\n",
       "      <td>0.587184</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>1.592820</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>1.360433</td>\n",
       "      <td>-1.650000</td>\n",
       "      <td>1.795504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>-1.675000</td>\n",
       "      <td>3.005204</td>\n",
       "      <td>115.450000</td>\n",
       "      <td>19.800000</td>\n",
       "      <td>229.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>115.450000</td>\n",
       "      <td>19.800000</td>\n",
       "      <td>229.800000</td>\n",
       "      <td>0.627566</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>1.592820</td>\n",
       "      <td>-0.150000</td>\n",
       "      <td>1.632944</td>\n",
       "      <td>-2.875000</td>\n",
       "      <td>0.858778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>-2.225000</td>\n",
       "      <td>1.873833</td>\n",
       "      <td>110.850000</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>205.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>110.850000</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>205.400000</td>\n",
       "      <td>0.238107</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.946410</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>1.616476</td>\n",
       "      <td>-2.225000</td>\n",
       "      <td>0.345783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>3.146625</td>\n",
       "      <td>107.250000</td>\n",
       "      <td>61.600000</td>\n",
       "      <td>193.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>107.250000</td>\n",
       "      <td>61.600000</td>\n",
       "      <td>193.400000</td>\n",
       "      <td>0.456164</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1.292820</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>1.184335</td>\n",
       "      <td>-1.575000</td>\n",
       "      <td>1.824541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>2.262742</td>\n",
       "      <td>125.600000</td>\n",
       "      <td>33.800000</td>\n",
       "      <td>237.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>125.600000</td>\n",
       "      <td>33.800000</td>\n",
       "      <td>237.600000</td>\n",
       "      <td>0.278467</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>1.592820</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.933638</td>\n",
       "      <td>-1.825000</td>\n",
       "      <td>1.633773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.275000</td>\n",
       "      <td>4.419417</td>\n",
       "      <td>150.650000</td>\n",
       "      <td>60.600000</td>\n",
       "      <td>280.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>150.650000</td>\n",
       "      <td>60.600000</td>\n",
       "      <td>280.400000</td>\n",
       "      <td>0.234009</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1.639230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.076459</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>3.696261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>3.889087</td>\n",
       "      <td>176.850000</td>\n",
       "      <td>55.600000</td>\n",
       "      <td>417.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>176.850000</td>\n",
       "      <td>55.600000</td>\n",
       "      <td>417.200000</td>\n",
       "      <td>0.221755</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>1.339230</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>1.620937</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>2.956115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>3.429468</td>\n",
       "      <td>267.450000</td>\n",
       "      <td>33.400000</td>\n",
       "      <td>861.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>83.816667</td>\n",
       "      <td>33.400000</td>\n",
       "      <td>161.400000</td>\n",
       "      <td>0.323324</td>\n",
       "      <td>No Log</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>1.432177</td>\n",
       "      <td>-1.400000</td>\n",
       "      <td>2.341827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRPO model saved to grpo_model\n"
     ]
    }
   ],
   "source": [
    "# Then do GRPO\n",
    "print(f\"\\nDoing GRPO training...\")\n",
    "grpo_dataset = prepare_grpo_dataset(gsm8k_train, RL_MODEL_TRAIN_SAMPLES)\n",
    "grpo_model = train_grpo_model(rl_sft_model, rl_tokenizer, grpo_dataset, \"grpo_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2102fea4-f399-4ef2-8bd5-ee767d191cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating GRPO model...\n",
      "Evaluating model on 512 examples with batch size 32...\n",
      "Loading LoRA adapter from grpo_model\n",
      "Processing batch 1/16 (examples 0-31)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e207f7a402244719fb70763b96fff01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b070c384d6a1458fb39d8c44ed74d7fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 2/16 (examples 32-63)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b240af33d0734e01bb48cd6dd3df0f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f3a1ed28f842edb67923d29eb7c161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 3/16 (examples 64-95)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2623c35e8401450287ac4e330435be2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db7174161c8b4cc8b778b9ddc5439f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 4/16 (examples 96-127)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0aec074b6a4275b8bd3f6a079b607d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66bb3ef347e34442936c24d696455528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 5/16 (examples 128-159)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7568788c9143efb69b1cbbbd67f426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fcec5a75449404db2472eeca6debe98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 6/16 (examples 160-191)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b728fffb5b4604b9b4089ebb03b85d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00bf511707f4b3aa47a065b68a18388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 7/16 (examples 192-223)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d815ed0e709148c194527977ee911b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76607aa5e64474f964069335e9f8799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 8/16 (examples 224-255)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f04107b767e47eba957142d6a6e6985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9b840f0e024430b5a8023564139ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 9/16 (examples 256-287)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a16e12f1c24fee8c83cb8ea71440bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43fc26d5f8b1440496470ef8ec264e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 10/16 (examples 288-319)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929baf58bf7c471ba6a277ec966c2341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3efbd31e01a941c6b4341729aaae5fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                        | 0/32 [00:00<?, ?it/s, est. sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nEvaluating GRPO model...\")\n",
    "grpo_accuracy, grpo_ft_thinking_prop, grpo_answer_prop = evaluate_model(\n",
    "    grpo_model, rl_tokenizer, gsm8k_test, EVAL_N, \"grpo_model_eval.csv\", \"grpo_model\"\n",
    ")\n",
    "\n",
    "print(f\"\\GPO model results:\")\n",
    "print(f\"Accuracy: {grpo_accuracy:.3f}\")\n",
    "print(f\"Thinking proportion: {grpo_ft_thinking_prop:.3f}\")\n",
    "print(f\"Answer proportion: {grpo_answer_prop:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f406b694-1343-413b-a2a9-853718a0a730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
